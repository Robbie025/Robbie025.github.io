---
layout: post
title: "Social Concerns with AI Automation"
---

# TLDR

There are good reasons to remove barriers to learning. It would not be wise to jump over the process of learning itself.


# AI Safety

 Machine Learning is the mathematics of data-prediction which is then construed as Artificial Intelligence. Figuring out why AI is dangerous is not very straightforward as those with good technical knowledge of ML systems seems very divided on this matter. I have listened to some of the arguments and I want to describe the general idea here and talk about why we are here in the first place.

# What others say about AI ?

Two guests from Lex Fridman's podcast standout for me - Bryan Jonson and Joshua Bach,  They have very different backgrounds and have interesting takes on many aspects of human life.

## Automation of Intelligence 
Bryan says -- 'the cost for design, production and distribution of intelligence is converging to zero. Currently, we are trying to negotiate the path forward'". 

This is a forward thinking statement and as of 2024 is not fully realized. I am not sure what he means by intelligence and it feels like he is working with a very minimal notion of what is Intelligence.  Indeed, the cost trend - the cost for training ML systems year over year is getting reduced by about 60%. These estimates vary widely, however i have not read of an increasing cost-trend.  

What does the "path forward", that Bryan refers to, look like? Well, there is the AI act in Europe as well as regulatory work in the US, China as well as other countries. Indeed South Korea, a few years ago reduced the tax benefits for automation. There is also a lot of research work being done on AI alignment as well.  

Note: Recently, i was adviced by an Innovation advisor in the medical sector that if there is AI in your device, it might be illegal. I thought then and still do that is rediculous.

## Automation of Learning
Joshua, a cognitive scientist have a very different concern. Students (that will end up in the workforce) are using ChatGPT to write their essays in schools. His concern lies in Writing, a pedagogical tool, that forces those who use this tool to structure their thoughts and express themselves. He points out that ChatGPT is essentially automating this important cognitive skill.

Note: This means that AI systems are capable of automating skills and not task and therefore an unintended concequence.

This is an AI-related safety issue akin to the inability for most people to carry out basic arithmetic operations without the use of a calculator. Have things become worse off because of the invention of a calculator? Anyhow, those who did not grow up with a calculator in school seemingly have lost this skill later on in life.

Anyhow, a calculator and an AI system are not the same. I dug a bit more deeper into what Joschua could have probably meant. The act of acquiring knowledge and making sense of it (Intelligence) is tough and is significant work. This is a learning process. 

### Learnings from History and Philosophy

NOTE: After writing this section, I notice BS spread all over it.

 Plato explains in the allegory of the cave (Book 7, The Republic) that it takes significant work to change our perception of the world with new knowledge. The reason I quote an ancient philosopher is to highlight the difficulty in processing new facts about your world and making sense of what has been observed. Additionally, Plato's observation seems very relevant — a potential hazard — for those attempting to automate the design, production, and distribution of knowledge. I am not capable of explaining this in a good way, but i give it a try here.

What do I mean by a hazard? I am refering to our ability to make sense of the world and communicate our ideas. The most accepted definition of knowledge is "justified belief" and with this defintiion in mind, we can acknowledge that with new data and its interpretation (a skill characteristic of Intelligence) results in paradigm shifts. As an example, the sun as the center of our solar system was the result dilligent data collecting and interpretion. This in turn led to a measurable explanation of seasons.  DNA encodes all required information of a living organism which in turn supported the notion that all humans are essentially the same.  

These examples might be too abstract for what i am trying to say. But at   a lower abstract level, if you writing an essay, and as you reiterate you will realize quality and well as understanding of what you are trying to convey improves. I would even say that if you verbally repeat your self, your message will be clearer over time. This is the underlying reason for politicians as well as professors to repeat themself. In this respect, AI systems (LLM's) are tools to spread knowledge and their (potential) effectives seems very scary to me. It is scary as it can spread knowledge as facts that are not accurate or correct, which is currently a well understood safety problem. If I were to use the language of Richard Dawkins - Large Language Models are very productive Meme Machines. 



# Final Thoughts 

As i write this, it is obvious to me that I am mostly ignorant about this topics. Joshu's concern can be very valid in a world where people simply accept information on what they should do and act in their daily lives from AI systems and dont take responsibility for it. We don't live in this world right now, but I have read about such behaviour already. e.g errors made by AI-assisted medical diagnostic systems, AI assisted law interpretation systems. My guess  is that as this is a developing phenomenon and those that are building AI systems are not completely aware of the hazards posed by their tools.  The question develops in a vicious cycle - how can you recognize hazards when you are posed with data when the skill to interpret the data has been automated away. 